#!/usr/bin/env python3
"""
ä¸¤å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œ - CIFAR-10 åˆ†ç±»å®éªŒ

å®Œæ•´çš„å®éªŒæµç¨‹ï¼š
1. åŠ è½½CIFAR-10æ•°æ®é›†
2. æ•°æ®é¢„å¤„ç†
3. è¶…å‚æ•°æœç´¢ï¼ˆå¯é€‰ï¼‰
4. è®­ç»ƒä¸¤å±‚ç¥ç»ç½‘ç»œï¼ˆå¸¦å­¦ä¹ ç‡è¡°å‡ï¼‰
5. è¯„ä¼°æ¨¡å‹æ€§èƒ½
6. ç”Ÿæˆä¸“ä¸šå¯è§†åŒ–ç»“æœ

æ”¹è¿›ç‚¹ï¼š
- Heæƒé‡åˆå§‹åŒ–ï¼Œæ”¶æ•›æ›´å¿«
- å­¦ä¹ ç‡è¡°å‡è°ƒåº¦
- Dropoutæ­£åˆ™åŒ–
- è¯¦ç»†çš„æ€§èƒ½åˆ†æ
- ä¸“ä¸šçš„ç»“æœå‘ˆç°
"""

from pathlib import Path

import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
import time
import pickle
from sklearn.metrics import confusion_matrix
import seaborn as sns

from two_layer_network.two_layer_network import TwoLayerNetwork
from .cifar10_utils import load_cifar10, preprocess_data, get_cifar10_class_names


# ============================================================================
# å®éªŒé…ç½® - åœ¨è¿™é‡Œä¿®æ”¹æ‰€æœ‰è¶…å‚æ•°
# ============================================================================
CONFIG = {
    # æ•°æ®é›†é…ç½®
    'data': {
        'train_samples': 40000,
        'val_samples': 5000,
        'test_samples': 5000,
    },
    
    # æ¨¡å‹é…ç½®
    'model': {
        'weight_scale': None,         # Noneä½¿ç”¨Heåˆå§‹åŒ–
        'dropout': 0.5,               # Dropoutæ¯”ç‡
    },
    
    # è®­ç»ƒé…ç½®
    'training': {
        'learning_rate_init': 1e-2,   # åˆå§‹å­¦ä¹ ç‡
        'num_epochs': 400,            # è®­ç»ƒè½®æ•°
        'batch_size': 256,            # æ‰¹æ¬¡å¤§å°
        'patience': 80,               # æ—©åœè€å¿ƒå€¼
        'print_every': 20,            # æ‰“å°é—´éš”
        'lr_decay_epochs': 50,        # å­¦ä¹ ç‡è¡°å‡é—´éš”
        'lr_decay_factor': 0.95,      # å­¦ä¹ ç‡è¡°å‡å› å­
    },
    
    # è¶…å‚æ•°æœç´¢é…ç½®
    'hyperparam_search': {
        'do_search': False,           # æ˜¯å¦æ‰§è¡Œæœç´¢
        'hidden_sizes': [100, 150, 200, 250],
        'regularizations': [1e-3, 5e-3, 1e-2, 5e-2],
        'learning_rates': [5e-3, 1e-2, 2e-2, 5e-2],
        'search_epochs': 200,         # æ¯ä¸ªç»„åˆè®­ç»ƒè½®æ•°
    },
    
    # é»˜è®¤è¶…å‚æ•°ï¼ˆä¸æœç´¢æ—¶ä½¿ç”¨ï¼‰
    'model_params': {
        'hidden_size': 200,
        'reg': 5e-3,
    },
}
# ============================================================================


def print_config():
    """æ‰“å°å®éªŒé…ç½®"""
    print("\n" + "=" * 80)
    print(" " * 25 + "CIFAR-10 å®éªŒé…ç½®")
    print("=" * 80)
    
    print("\nğŸ“Š æ•°æ®é›†é…ç½®:")
    for key, value in CONFIG['data'].items():
        print(f"   â€¢ {key:25s}: {value:,}")
    
    print("\nğŸ§  æ¨¡å‹é…ç½®:")
    for key, value in CONFIG['model'].items():
        print(f"   â€¢ {key:25s}: {value}")
    
    print("\nğŸ¯ è®­ç»ƒé…ç½®:")
    for key, value in CONFIG['training'].items():
        print(f"   â€¢ {key:25s}: {value}")
    
    if CONFIG['hyperparam_search']['do_search']:
        print("\nğŸ” è¶…å‚æ•°æœç´¢:")
        for key, value in CONFIG['hyperparam_search'].items():
            if key != 'do_search':
                print(f"   â€¢ {key:25s}: {value}")
    else:
        print("\nğŸ” ä½¿ç”¨é»˜è®¤æ¨¡å‹å‚æ•°:")
        for key, value in CONFIG['model_params'].items():
            print(f"   â€¢ {key:25s}: {value}")
    
    print("\n" + "=" * 80 + "\n")

def cross_validation(X_train, y_train, X_val, y_val):
    """
    è¶…å‚æ•°æœç´¢ - ä½¿ç”¨éªŒè¯é›†è¯„ä¼°
    
    å‚æ•°ï¼š
        X_train, y_train: è®­ç»ƒæ•°æ®
        X_val, y_val: éªŒè¯æ•°æ®
    
    è¿”å›ï¼š
        results: æ‰€æœ‰è¶…å‚æ•°ç»„åˆçš„ç»“æœ
        best_params: æœ€ä¼˜è¶…å‚æ•°
        best_val_acc: æœ€ä¼˜éªŒè¯å‡†ç¡®ç‡
    """
    cfg = CONFIG['hyperparam_search']
    
    hidden_sizes = cfg['hidden_sizes']
    regularizations = cfg['regularizations']
    learning_rates = cfg['learning_rates']
    num_epochs = cfg['search_epochs']
    
    results = {}
    best_val_acc = 0
    best_params = None
    
    print("\n" + "=" * 80)
    print("ğŸ” å¼€å§‹è¶…å‚æ•°æœç´¢")
    print("=" * 80)
    print(f"éšè—å±‚å¤§å°: {hidden_sizes}")
    print(f"æ­£åˆ™åŒ–ç³»æ•°: {regularizations}")
    print(f"å­¦ä¹ ç‡: {learning_rates}")
    print(f"æ¯ä¸ªç»„åˆè®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"æ€»æœç´¢ç»„åˆæ•°: {len(hidden_sizes) * len(regularizations) * len(learning_rates)}")
    print("=" * 80)
    
    total = len(hidden_sizes) * len(regularizations) * len(learning_rates)
    current = 0
    
    for h in hidden_sizes:
        for reg in regularizations:
            for lr in learning_rates:
                current += 1
                
                # åˆ›å»ºç½‘ç»œ
                net = TwoLayerNetwork(
                    input_size=X_train.shape[1],
                    hidden_size=h,
                    num_classes=10,
                    weight_scale=CONFIG['model']['weight_scale'],
                    reg=reg,
                    dropout=CONFIG['model']['dropout']
                )
                
                # è®­ç»ƒ
                print(f"\n[{current}/{total}] H={h:3d}, Î»={reg:.0e}, lr={lr:.0e}", end='')
                
                best_epoch_acc = 0
                for epoch in range(num_epochs):
                    # å°æ‰¹é‡è®­ç»ƒ
                    indices = np.random.choice(len(X_train), 
                                              min(256, len(X_train)), 
                                              replace=False)
                    net.train_step(X_train[indices], y_train[indices], lr)
                    
                    if (epoch + 1) % max(1, num_epochs // 5) == 0:
                        val_acc = net.evaluate(X_val, y_val)
                        print(f" â†’ Epoch {epoch+1}: val_acc={val_acc:.4f}", end='')
                        best_epoch_acc = max(best_epoch_acc, val_acc)
                
                results[(h, reg, lr)] = best_epoch_acc
                
                if best_epoch_acc > best_val_acc:
                    best_val_acc = best_epoch_acc
                    best_params = {
                        'hidden_size': h,
                        'reg': reg,
                        'learning_rate': lr
                    }
                    print(" âœ“ æ–°æœ€ä¼˜ï¼")
    
    print("\n" + "=" * 80)
    print(f"âœ… è¶…å‚æ•°æœç´¢å®Œæˆï¼")
    print(f"æœ€ä¼˜å‚æ•°: {best_params}")
    print(f"æœ€ä¼˜éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)")
    print("=" * 80)
    
    return results, best_params, best_val_acc
def train_with_decay(net, X_train, y_train, X_val, y_val, best_params=None):
    """
    å¸¦å­¦ä¹ ç‡è¡°å‡çš„è®­ç»ƒ
    
    å‚æ•°ï¼š
        net: ç¥ç»ç½‘ç»œæ¨¡å‹
        X_train, y_train: è®­ç»ƒæ•°æ®
        X_val, y_val: éªŒè¯æ•°æ®
        best_params: æœ€ä¼˜è¶…å‚æ•°ï¼ˆåŒ…å«learning_rateï¼‰
    
    è¿”å›ï¼š
        history: è®­ç»ƒå†å²
        best_epoch: æœ€ä¼˜è½®æ•°
        best_model_params: æœ€ä¼˜æ¨¡å‹å‚æ•°
    """
    cfg = CONFIG['training']
    
    if best_params is not None:
        lr_init = best_params.get('learning_rate', cfg['learning_rate_init'])
    else:
        lr_init = cfg['learning_rate_init']
    
    num_epochs = cfg['num_epochs']
    batch_size = cfg['batch_size']
    patience = cfg['patience']
    print_every = cfg['print_every']
    lr_decay_epochs = cfg['lr_decay_epochs']
    lr_decay_factor = cfg['lr_decay_factor']
    
    history = {
        'epochs': [],
        'train_loss': [],
        'train_acc': [],
        'val_acc': [],
        'learning_rates': [],
    }
    
    best_val_acc = 0
    best_epoch = 0
    best_model_params = None
    epochs_no_improve = 0
    lr = lr_init
    
    num_batches = len(X_train) // batch_size
    
    print("\n" + "=" * 80)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ")
    print("=" * 80)
    print(f"åˆå§‹å­¦ä¹ ç‡: {lr_init:.0e}")
    print(f"æ€»è½®æ•°: {num_epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"æ—©åœè€å¿ƒå€¼: {patience}")
    print("=" * 80)
    
    start_time = time.time()
    
    for epoch in range(num_epochs):
        # å­¦ä¹ ç‡è¡°å‡
        if epoch > 0 and epoch % lr_decay_epochs == 0:
            lr *= lr_decay_factor
        
        # è®­ç»ƒä¸€ä¸ªepoch
        epoch_loss = 0
        indices = np.random.permutation(len(X_train))
        
        for i in range(0, len(X_train), batch_size):
            batch_indices = indices[i:i + batch_size]
            X_batch = X_train[batch_indices]
            y_batch = y_train[batch_indices]
            
            loss = net.train_step(X_batch, y_batch, lr)
            epoch_loss += loss
        
        epoch_loss /= num_batches
        
        # è®¡ç®—è®­ç»ƒå’ŒéªŒè¯ç²¾åº¦
        train_acc = net.evaluate(X_train, y_train)
        val_acc = net.evaluate(X_val, y_val)
        
        # è®°å½•å†å²
        history['epochs'].append(epoch + 1)
        history['train_loss'].append(epoch_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        history['learning_rates'].append(lr)
        
        # æ—©åœæ£€æŸ¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_epoch = epoch + 1
            best_model_params = {k: v.copy() for k, v in net.params.items()}
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % print_every == 0 or epoch == 0:
            print(f"Epoch {epoch+1:3d}/{num_epochs} | "
                  f"Loss: {epoch_loss:.4f} | "
                  f"Train Acc: {train_acc:.4f} | "
                  f"Val Acc: {val_acc:.4f} | "
                  f"LR: {lr:.0e}")
        
        # æ—©åœ
        if epochs_no_improve >= patience:
            print(f"\nâ¹ï¸  æ—©åœï¼åœ¨ç¬¬ {epoch+1} è½®è¾¾åˆ°æœ€ä¼˜éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
            break
    
    elapsed_time = time.time() - start_time
    
    print("=" * 80)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"    æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"    æœ€ä¼˜è½®æ•°: {best_epoch}")
    print(f"    æœ€ä¼˜éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)")
    print("=" * 80)
    
    # æ¢å¤æœ€ä¼˜æ¨¡å‹å‚æ•°
    if best_model_params is not None:
        net.params = best_model_params
    
    return history, best_epoch, best_model_params, elapsed_time


def plot_training_curves(history, results_dir):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆæŸå¤±ã€å‡†ç¡®ç‡ã€å­¦ä¹ ç‡ï¼‰
    
    å‚æ•°ï¼š
        history: è®­ç»ƒå†å²å­—å…¸
        results_dir: ç»“æœä¿å­˜ç›®å½•
    """
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    epochs = history['epochs']
    
    # 1. æŸå¤±æ›²çº¿
    axes[0].plot(epochs, history['train_loss'], 'b-', linewidth=2.5, 
                 label='Training Loss', marker='o', markersize=4)
    axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')
    axes[0].set_title('Training Loss', fontsize=13, fontweight='bold')
    axes[0].grid(True, alpha=0.3, linestyle='--')
    axes[0].legend(fontsize=11)
    
    # 2. å‡†ç¡®ç‡æ›²çº¿
    axes[1].plot(epochs, history['train_acc'], 'b-', linewidth=2.5,
                 label='Training Accuracy', marker='o', markersize=4)
    axes[1].plot(epochs, history['val_acc'], 'r-', linewidth=2.5,
                 label='Validation Accuracy', marker='s', markersize=4)
    axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')
    axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')
    axes[1].set_title('Training & Validation Accuracy', fontsize=13, fontweight='bold')
    axes[1].grid(True, alpha=0.3, linestyle='--')
    axes[1].legend(fontsize=11, loc='lower right')
    axes[1].set_ylim([0, 1])
    
    # 3. å­¦ä¹ ç‡å˜åŒ–
    axes[2].plot(epochs, history['learning_rates'], 'g-', linewidth=2.5, marker='D', markersize=4)
    axes[2].set_xlabel('Epoch', fontsize=12, fontweight='bold')
    axes[2].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')
    axes[2].set_title('Learning Rate Schedule', fontsize=13, fontweight='bold')
    axes[2].grid(True, alpha=0.3, linestyle='--')
    axes[2].set_yscale('log')
    
    plt.suptitle('Training Curves - Two Layer Network', fontsize=15, fontweight='bold', y=1.00)
    plt.tight_layout()
    
    save_path = results_dir / 'training_curves.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path.name}")
    plt.close()


def plot_confusion_matrix(y_true, y_pred, class_names, results_dir):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    
    å‚æ•°ï¼š
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        results_dir: ç»“æœä¿å­˜ç›®å½•
    """
    cm = confusion_matrix(y_true, y_pred)
    
    # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # ç»˜åˆ¶åŸå§‹æ··æ·†çŸ©é˜µ
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Count'})
    ax1.set_title('Confusion Matrix (Raw Count)', fontsize=13, fontweight='bold')
    ax1.set_ylabel('True Label', fontsize=11, fontweight='bold')
    ax1.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')
    
    # ç»˜åˆ¶å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens', ax=ax2,
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Accuracy'}, vmin=0, vmax=1)
    ax2.set_title('Confusion Matrix (Normalized)', fontsize=13, fontweight='bold')
    ax2.set_ylabel('True Label', fontsize=11, fontweight='bold')
    ax2.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')
    
    plt.suptitle('Confusion Matrix Analysis', fontsize=15, fontweight='bold')
    plt.tight_layout()
    
    save_path = results_dir / 'confusion_matrix.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path.name}")
    plt.close()
    
    # è®¡ç®—æ¯ç±»å‡†ç¡®ç‡
    class_accuracies = cm_normalized.diagonal()
    
    return class_accuracies


def plot_per_class_accuracy(class_accuracies, class_names, results_dir):
    """
    ç»˜åˆ¶æ¯ç±»å‡†ç¡®ç‡æŸ±çŠ¶å›¾
    
    å‚æ•°ï¼š
        class_accuracies: æ¯ç±»å‡†ç¡®ç‡æ•°ç»„
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        results_dir: ç»“æœä¿å­˜ç›®å½•
    """
    fig, ax = plt.subplots(figsize=(12, 6))
    
    colors = plt.cm.RdYlGn(class_accuracies)
    bars = ax.bar(range(len(class_names)), class_accuracies, color=colors, 
                  edgecolor='black', linewidth=1.5)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, acc) in enumerate(zip(bars, class_accuracies)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{acc:.1%}',
                ha='center', va='bottom', fontweight='bold', fontsize=10)
    
    ax.set_xlabel('Class', fontsize=12, fontweight='bold')
    ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')
    ax.set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')
    ax.set_xticks(range(len(class_names)))
    ax.set_xticklabels(class_names, rotation=45, ha='right')
    ax.set_ylim([0, 1])
    ax.grid(True, alpha=0.3, axis='y', linestyle='--')
    
    # æ·»åŠ å¹³å‡çº¿
    mean_acc = np.mean(class_accuracies)
    ax.axhline(y=mean_acc, color='red', linestyle='--', linewidth=2, 
               label=f'Mean: {mean_acc:.1%}')
    ax.legend(fontsize=11)
    
    plt.tight_layout()
    
    save_path = results_dir / 'per_class_accuracy.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ æ¯ç±»å‡†ç¡®ç‡å·²ä¿å­˜: {save_path.name}")
    plt.close()


def visualize_predictions(X_test, y_test, net, class_names, results_dir, num_samples=20):
    """
    å¯è§†åŒ–é¢„æµ‹ç»“æœï¼ˆæ­£ç¡®å’Œé”™è¯¯ï¼‰
    
    å‚æ•°ï¼š
        X_test: æµ‹è¯•æ•°æ®
        y_test: æµ‹è¯•æ ‡ç­¾
        net: è®­ç»ƒå¥½çš„ç½‘ç»œ
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        results_dir: ç»“æœä¿å­˜ç›®å½•
        num_samples: å¯è§†åŒ–æ ·æœ¬æ•°
    """
    predictions = net.predict(X_test)
    
    # åˆ†ç¦»æ­£ç¡®å’Œé”™è¯¯æ ·æœ¬
    correct_mask = predictions == y_test
    incorrect_mask = ~correct_mask
    
    correct_indices = np.where(correct_mask)[0]
    incorrect_indices = np.where(incorrect_mask)[0]
    
    # é‡‡æ ·
    num_correct = min(10, len(correct_indices))
    num_incorrect = min(10, len(incorrect_indices))
    
    correct_samples = np.random.choice(correct_indices, num_correct, replace=False)
    incorrect_samples = np.random.choice(incorrect_indices, num_incorrect, replace=False)
    
    fig, axes = plt.subplots(2, 10, figsize=(16, 4))
    
    # ç»˜åˆ¶æ­£ç¡®é¢„æµ‹
    for idx, sample_idx in enumerate(correct_samples):
        ax = axes[0, idx]
        img = X_test[sample_idx].reshape(32, 32, 3)
        img = (img - img.min()) / (img.max() - img.min() + 1e-5)
        img = np.clip(img, 0, 1)
        
        ax.imshow(img)
        true_label = class_names[y_test[sample_idx]]
        ax.set_title(f'{true_label}', color='green', fontweight='bold', fontsize=9)
        ax.axis('off')
    
    # ç»˜åˆ¶é”™è¯¯é¢„æµ‹
    for idx, sample_idx in enumerate(incorrect_samples):
        ax = axes[1, idx]
        img = X_test[sample_idx].reshape(32, 32, 3)
        img = (img - img.min()) / (img.max() - img.min() + 1e-5)
        img = np.clip(img, 0, 1)
        
        ax.imshow(img)
        true_label = class_names[y_test[sample_idx]]
        pred_label = class_names[predictions[sample_idx]]
        title = f'True: {true_label}\nPred: {pred_label}'
        ax.set_title(title, color='red', fontweight='bold', fontsize=8)
        ax.axis('off')
    
    axes[0, 0].text(-0.5, 0.5, 'Correct Predictions', transform=axes[0, 0].transAxes,
                     fontsize=11, fontweight='bold', rotation=90, va='center')
    axes[1, 0].text(-0.5, 0.5, 'Wrong Predictions', transform=axes[1, 0].transAxes,
                     fontsize=11, fontweight='bold', rotation=90, va='center')
    
    plt.suptitle('Sample Predictions (  Green=Correct, Red=Wrong)', 
                 fontsize=13, fontweight='bold')
    plt.tight_layout()
    
    save_path = results_dir / 'predictions_visualization.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ é¢„æµ‹å¯è§†åŒ–å·²ä¿å­˜: {save_path.name}")
    plt.close()


def visualize_learned_features(net, class_names, results_dir):
    """
    å¯è§†åŒ–å­¦ä¹ åˆ°çš„éšè—å±‚ç‰¹å¾æƒé‡
    
    å‚æ•°ï¼š
        net: è®­ç»ƒå¥½çš„ç½‘ç»œ
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        results_dir: ç»“æœä¿å­˜ç›®å½•
    """
    W1 = net.params['W1']
    
    # é€‰æ‹©æœ€æœ‰ä»£è¡¨æ€§çš„32ä¸ªç¥ç»å…ƒï¼ˆåŸºäºæƒé‡èŒƒæ•°ï¼‰
    weight_norms = np.linalg.norm(W1, axis=0)
    top_indices = np.argsort(weight_norms)[-32:][::-1]
    
    fig, axes = plt.subplots(4, 8, figsize=(14, 7))
    axes = axes.flatten()
    
    for idx, neuron_idx in enumerate(top_indices):
        w = W1[:, neuron_idx].reshape(32, 32, 3)
        
        # å½’ä¸€åŒ–åˆ°[0, 1]
        w_normalized = (w - w.min()) / (w.max() - w.min() + 1e-5)
        w_normalized = np.clip(w_normalized, 0, 1)
        
        axes[idx].imshow(w_normalized)
        axes[idx].set_title(f'Neuron {neuron_idx}', fontsize=9, fontweight='bold')
        axes[idx].axis('off')
    
    plt.suptitle('Learned Hidden Layer Features (Top 32 by Norm)', 
                 fontsize=13, fontweight='bold')
    plt.tight_layout()
    
    save_path = results_dir / 'learned_features.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ å­¦ä¹ ç‰¹å¾å¯è§†åŒ–å·²ä¿å­˜: {save_path.name}")
    plt.close()


def save_results(net, history, train_acc, val_acc, test_acc, 
                 class_accuracies, class_names, elapsed_time, results_dir):
    """
    ä¿å­˜æ‰€æœ‰å®éªŒç»“æœ
    
    å‚æ•°ï¼š
        net: è®­ç»ƒå¥½çš„ç½‘ç»œ
        history: è®­ç»ƒå†å²
        train_acc, val_acc, test_acc: å‡†ç¡®ç‡
        class_accuracies: æ¯ç±»å‡†ç¡®ç‡
        class_names: ç±»åˆ«åç§°
        elapsed_time: è®­ç»ƒè€—æ—¶
        results_dir: ç»“æœä¿å­˜ç›®å½•
    """
    # 1. ä¿å­˜æ¨¡å‹
    model_path = results_dir / 'best_model.pkl'
    net.save_model(model_path)
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path.name}")
    
    # 2. ä¿å­˜è®­ç»ƒå†å²
    history_path = results_dir / 'training_history.pkl'
    with open(history_path, 'wb') as f:
        pickle.dump(history, f)
    print(f"âœ“ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path.name}")
    
    # 3. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_path = results_dir / 'experiment_report.txt'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write(" " * 20 + "CIFAR-10 å®éªŒæŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        # æ¨¡å‹é…ç½®
        f.write("ã€æ¨¡å‹é…ç½®ã€‘\n")
        f.write(f"  è¾“å…¥ç»´åº¦: {net.input_size}\n")
        f.write(f"  éšè—å±‚ç»´åº¦: {net.hidden_size}\n")
        f.write(f"  è¾“å‡ºç»´åº¦: {net.num_classes}\n")
        f.write(f"  æ­£åˆ™åŒ–ç³»æ•°: {net.reg}\n")
        f.write(f"  Dropout: {net.dropout}\n")
        f.write(f"  æ€»å‚æ•°é‡: {net.input_size * net.hidden_size + net.hidden_size * net.num_classes:,}\n\n")
        
        # è®­ç»ƒé…ç½®
        f.write("ã€è®­ç»ƒé…ç½®ã€‘\n")
        f.write(f"  è®­ç»ƒæ ·æœ¬æ•°: {CONFIG['data']['train_samples']:,}\n")
        f.write(f"  éªŒè¯æ ·æœ¬æ•°: {CONFIG['data']['val_samples']:,}\n")
        f.write(f"  æµ‹è¯•æ ·æœ¬æ•°: {CONFIG['data']['test_samples']:,}\n")
        f.write(f"  åˆå§‹å­¦ä¹ ç‡: {CONFIG['training']['learning_rate_init']}\n")
        f.write(f"  æ‰¹æ¬¡å¤§å°: {CONFIG['training']['batch_size']}\n")
        f.write(f"  æ€»è®­ç»ƒè½®æ•°: {len(history['epochs'])}\n")
        f.write(f"  è®­ç»ƒè€—æ—¶: {elapsed_time:.2f} ç§’ ({elapsed_time/60:.2f} åˆ†é’Ÿ)\n\n")
        
        # æ€§èƒ½æŒ‡æ ‡
        f.write("ã€æ€§èƒ½æŒ‡æ ‡ã€‘\n")
        f.write(f"  è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f} ({train_acc*100:.2f}%)\n")
        f.write(f"  éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f} ({val_acc*100:.2f}%)\n")
        f.write(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)\n\n")
        
        # æ¯ç±»å‡†ç¡®ç‡
        f.write("ã€æ¯ç±»å‡†ç¡®ç‡ã€‘\n")
        for class_name, acc in zip(class_names, class_accuracies):
            f.write(f"  {class_name:12s}: {acc:.4f} ({acc*100:.2f}%)\n")
        f.write(f"  å¹³å‡å‡†ç¡®ç‡: {np.mean(class_accuracies):.4f} ({np.mean(class_accuracies)*100:.2f}%)\n\n")
        
        # æœ€ä¼˜å’Œæœ€å·®ç±»åˆ«
        best_class_idx = np.argmax(class_accuracies)
        worst_class_idx = np.argmin(class_accuracies)
        f.write(f"  æœ€ä¼˜ç±»åˆ«: {class_names[best_class_idx]} ({class_accuracies[best_class_idx]*100:.2f}%)\n")
        f.write(f"  æœ€å·®ç±»åˆ«: {class_names[worst_class_idx]} ({class_accuracies[worst_class_idx]*100:.2f}%)\n\n")
        
        # è®­ç»ƒå†å²æ‘˜è¦
        f.write("ã€è®­ç»ƒå†å²ã€‘\n")
        f.write(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {history['train_loss'][-1]:.4f}\n")
        f.write(f"  æœ€å°è®­ç»ƒæŸå¤±: {min(history['train_loss']):.4f}\n")
        f.write(f"  æœ€é«˜éªŒè¯å‡†ç¡®ç‡: {max(history['val_acc']):.4f} ({max(history['val_acc'])*100:.2f}%)\n")
        f.write(f"  æœ€ç»ˆå­¦ä¹ ç‡: {history['learning_rates'][-1]:.0e}\n\n")
        
        f.write("=" * 80 + "\n")
        f.write("å®éªŒå®Œæˆæ—¶é—´: " + time.strftime("%Y-%m-%d %H:%M:%S") + "\n")
        f.write("=" * 80 + "\n")
    
    print(f"âœ“ å®éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_path.name}")
    
    # 4. æ‰“å°æ‘˜è¦åˆ°æ§åˆ¶å°
    print("\n" + "=" * 80)
    print(" " * 28 + "å®éªŒç»“æœæ‘˜è¦")
    print("=" * 80)
    print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f} ({train_acc*100:.2f}%)")
    print(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f} ({val_acc*100:.2f}%)")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)")
    print(f"è®­ç»ƒè€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"å‚æ•°æ•°é‡: {net.input_size * net.hidden_size + net.hidden_size * net.num_classes:,}")
    print("=" * 80 + "\n")


def main():
    """ä¸»å®éªŒæµç¨‹"""
    
    # æ‰“å°é…ç½®
    print_config()
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = Path('results')
    results_dir.mkdir(exist_ok=True)
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜åˆ°: {results_dir.resolve()}\n")
    
    # ========================================================================
    # 1. åŠ è½½æ•°æ®
    # ========================================================================
    print("=" * 80)
    print("ğŸ“Š åŠ è½½ CIFAR-10 æ•°æ®é›†")
    print("=" * 80)
    
    data_dir = Path('data/cifar-10-batches-py')
    
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print("è¯·ç¡®ä¿CIFAR-10æ•°æ®é›†åœ¨æ­£ç¡®ä½ç½®")
        return
    
    X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(
        data_dir,
        train_samples=CONFIG['data']['train_samples'],
        val_samples=CONFIG['data']['val_samples'],
        test_samples=CONFIG['data']['test_samples']
    )
    
    print(f"âœ“ è®­ç»ƒé›†: {X_train.shape}")
    print(f"âœ“ éªŒè¯é›†: {X_val.shape}")
    print(f"âœ“ æµ‹è¯•é›†: {X_test.shape}")
    
    # ========================================================================
    # 2. æ•°æ®é¢„å¤„ç†
    # ========================================================================
    print("\n" + "=" * 80)
    print("ğŸ”§ æ•°æ®é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ– + ä¸­å¿ƒåŒ– + åç½®ï¼‰")
    print("=" * 80)
    
    X_train, X_val, X_test = preprocess_data(X_train, X_val, X_test)
    
    print(f"âœ“ é¢„å¤„ç†åè®­ç»ƒé›†: {X_train.shape}")
    print(f"âœ“ é¢„å¤„ç†åéªŒè¯é›†: {X_val.shape}")
    print(f"âœ“ é¢„å¤„ç†åæµ‹è¯•é›†: {X_test.shape}")
    
    class_names = get_cifar10_class_names()
    
    # ========================================================================
    # 3. è¶…å‚æ•°æœç´¢ï¼ˆå¯é€‰ï¼‰
    # ========================================================================
    best_params = None
    
    if CONFIG['hyperparam_search']['do_search']:
        results, best_params, best_val_acc = cross_validation(
            X_train, y_train, X_val, y_val
        )
        
        # ä½¿ç”¨æœç´¢åˆ°çš„æœ€ä¼˜å‚æ•°
        hidden_size = best_params['hidden_size']
        reg = best_params['reg']
    else:
        # ä½¿ç”¨é»˜è®¤å‚æ•°
        hidden_size = CONFIG['model_params']['hidden_size']
        reg = CONFIG['model_params']['reg']
        best_params = {
            'hidden_size': hidden_size,
            'reg': reg,
            'learning_rate': CONFIG['training']['learning_rate_init']
        }
    
    # ========================================================================
    # 4. åˆ›å»ºå¹¶è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    # ========================================================================
    print("\n" + "=" * 80)
    print("ğŸ§  åˆ›å»ºæœ€ç»ˆæ¨¡å‹")
    print("=" * 80)
    
    net = TwoLayerNetwork(
        input_size=X_train.shape[1],
        hidden_size=hidden_size,
        num_classes=10,
        weight_scale=CONFIG['model']['weight_scale'],
        reg=reg,
        dropout=CONFIG['model']['dropout']
    )
    
    print(f"âœ“ è¾“å…¥ç»´åº¦: {net.input_size}")
    print(f"âœ“ éšè—å±‚ç»´åº¦: {net.hidden_size}")
    print(f"âœ“ è¾“å‡ºç»´åº¦: {net.num_classes}")
    print(f"âœ“ æ­£åˆ™åŒ–ç³»æ•°: {net.reg}")
    print(f"âœ“ Dropout: {net.dropout}")
    print(f"âœ“ æ€»å‚æ•°é‡: {net.input_size * net.hidden_size + net.hidden_size * net.num_classes:,}")
    
    # è®­ç»ƒ
    history, best_epoch, best_model_params, elapsed_time = train_with_decay(
        net, X_train, y_train, X_val, y_val, best_params
    )
    
    # ========================================================================
    # 5. è¯„ä¼°æ¨¡å‹
    # ========================================================================
    print("\n" + "=" * 80)
    print("ğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½")
    print("=" * 80)
    
    train_acc = net.evaluate(X_train, y_train)
    val_acc = net.evaluate(X_val, y_val)
    test_acc = net.evaluate(X_test, y_test)
    
    print(f"âœ“ è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f} ({train_acc*100:.2f}%)")
    print(f"âœ“ éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f} ({val_acc*100:.2f}%)")
    print(f"âœ“ æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)")
    
    # ========================================================================
    # 6. ç”Ÿæˆå¯è§†åŒ–
    # ========================================================================
    print("\n" + "=" * 80)
    print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ")
    print("=" * 80)
    
    # è®­ç»ƒæ›²çº¿
    plot_training_curves(history, results_dir)
    
    # æ··æ·†çŸ©é˜µ
    y_test_pred = net.predict(X_test)
    class_accuracies = plot_confusion_matrix(y_test, y_test_pred, class_names, results_dir)
    
    # æ¯ç±»å‡†ç¡®ç‡
    plot_per_class_accuracy(class_accuracies, class_names, results_dir)
    
    # é¢„æµ‹å¯è§†åŒ–
    visualize_predictions(X_test, y_test, net, class_names, results_dir)
    
    # å­¦ä¹ ç‰¹å¾å¯è§†åŒ–
    visualize_learned_features(net, class_names, results_dir)
    
    # ========================================================================
    # 7. ä¿å­˜ç»“æœ
    # ========================================================================
    print("\n" + "=" * 80)
    print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ")
    print("=" * 80)
    
    save_results(net, history, train_acc, val_acc, test_acc,
                 class_accuracies, class_names, elapsed_time, results_dir)
    
    print("\nğŸ‰ å®éªŒå…¨éƒ¨å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")
    print("=" * 80 + "\n")


if __name__ == '__main__':
    main()