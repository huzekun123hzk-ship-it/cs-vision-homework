# rnn/experiments/compare_rnn_variants.py
"""
RNNå˜ä½“å¯¹æ¯”å®éªŒï¼šVanilla RNN vs GRU vs LSTM

å…¨é¢å¯¹æ¯”ä¸‰ç§æ¶æ„åœ¨CIFAR-10ä¸Šçš„è¡¨ç°
"""

from __future__ import annotations
import sys
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

current_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(current_dir.parent))

from rnn_classifier import RNNClassifier
from gru_classifier import GRUClassifier
from lstm_classifier import LSTMClassifier
from cifar10_utils import load_cifar10, preprocess_data


def reshape_for_rnn(X_flat: np.ndarray) -> np.ndarray:
    """(N, 3072) -> (N, 32, 96)"""
    return X_flat.reshape(X_flat.shape[0], 32, 96).astype(np.float32)


def load_history(path: Path):
    """åŠ è½½è®­ç»ƒå†å²"""
    if not path.exists():
        return None
    return np.load(str(path), allow_pickle=True).item()


def plot_comparison(histories: dict, save_dir: Path):
    """
    ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    histories: {"Vanilla RNN": history1, "GRU": history2, "LSTM": history3}
    """
    colors = {
        "Vanilla RNN": "#e74c3c",
        "GRU": "#3498db",
        "LSTM": "#2ecc71",
    }
    
    # 1. Losså¯¹æ¯”
    fig, ax = plt.subplots(figsize=(10, 6))
    for name, hist in histories.items():
        if hist is None:
            continue
        loss_epochs = sorted(hist["loss_history"].keys())
        losses = [hist["loss_history"][e] for e in loss_epochs]
        ax.plot(loss_epochs, losses, label=name, linewidth=2, color=colors.get(name, 'gray'))
    
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Loss', fontsize=12)
    ax.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    fig.tight_layout()
    fig.savefig(save_dir / 'comparison_loss.png', dpi=150)
    plt.close(fig)
    print("âœ“ Saved: comparison_loss.png")
    
    # 2. Validation Accuracyå¯¹æ¯”
    fig, ax = plt.subplots(figsize=(10, 6))
    for name, hist in histories.items():
        if hist is None:
            continue
        epochs = hist["epochs"]
        val_acc = hist["val_acc_history"]
        ax.plot(epochs, val_acc, label=name, linewidth=2.5, 
                marker='o', markersize=4, color=colors.get(name, 'gray'))
    
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Validation Accuracy', fontsize=12)
    ax.set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.set_ylim([0.3, 0.6])
    fig.tight_layout()
    fig.savefig(save_dir / 'comparison_val_acc.png', dpi=150)
    plt.close(fig)
    print("âœ“ Saved: comparison_val_acc.png")
    
    # 3. Gradient Normå¯¹æ¯”
    fig, ax = plt.subplots(figsize=(10, 6))
    for name, hist in histories.items():
        if hist is None:
            continue
        grad_norms = hist.get("grad_norm_history", [])
        if len(grad_norms) > 0:
            ax.plot(grad_norms, label=name, linewidth=1.5, alpha=0.7, color=colors.get(name, 'gray'))
    
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Gradient Norm', fontsize=12)
    ax.set_title('Gradient Norm Comparison', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    fig.tight_layout()
    fig.savefig(save_dir / 'comparison_grad_norm.png', dpi=150)
    plt.close(fig)
    print("âœ“ Saved: comparison_grad_norm.png")


def plot_final_results(results: dict, save_dir: Path):
    """
    ç»˜åˆ¶æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡æŸ±çŠ¶å›¾
    results: {"Vanilla RNN": 0.493, "GRU": 0.51, "LSTM": 0.52}
    """
    names = list(results.keys())
    accs = [results[n] * 100 for n in names]
    colors_list = ["#e74c3c", "#3498db", "#2ecc71"]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(names, accs, color=colors_list, edgecolor='black', linewidth=1.5, width=0.6)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, acc in zip(bars, accs):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                f'{acc:.2f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')
    
    ax.set_ylabel('Test Accuracy (%)', fontsize=12)
    ax.set_title('CIFAR-10 Test Accuracy: RNN Variants Comparison', 
                 fontsize=14, fontweight='bold')
    ax.set_ylim([0, 60])
    ax.grid(axis='y', alpha=0.3)
    
    fig.tight_layout()
    fig.savefig(save_dir / 'comparison_final_results.png', dpi=150)
    plt.close(fig)
    print("âœ“ Saved: comparison_final_results.png")


def generate_report(results: dict, histories: dict, save_path: Path):
    """ç”Ÿæˆæ–‡å­—æŠ¥å‘Š"""
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("RNN Variants Comparison Report - CIFAR-10\n")
        f.write("=" * 70 + "\n\n")
        
        f.write("1. Final Test Accuracy\n")
        f.write("-" * 70 + "\n")
        for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):
            f.write(f"   {name:15s}: {acc:.4f} ({acc*100:.2f}%)\n")
        
        f.write("\n2. Training Summary\n")
        f.write("-" * 70 + "\n")
        for name, hist in histories.items():
            if hist is None:
                f.write(f"\n{name}: No data available\n")
                continue
            
            best_val = max(hist["val_acc_history"])
            final_val = hist["val_acc_history"][-1] if len(hist["val_acc_history"]) > 0 else 0
            total_epochs = len(hist["loss_history"])
            
            f.write(f"\n{name}:\n")
            f.write(f"   Total Epochs:       {total_epochs}\n")
            f.write(f"   Best Val Acc:       {best_val:.4f}\n")
            f.write(f"   Final Val Acc:      {final_val:.4f}\n")
            f.write(f"   Test Acc:           {results.get(name, 0):.4f}\n")
        
        f.write("\n" + "=" * 70 + "\n")
        f.write("Key Findings:\n")
        f.write("-" * 70 + "\n")
        
        best_model = max(results.items(), key=lambda x: x[1])[0]
        worst_model = min(results.items(), key=lambda x: x[1])[0]
        
        f.write(f"- Best Model:  {best_model} ({results[best_model]*100:.2f}%)\n")
        f.write(f"- Worst Model: {worst_model} ({results[worst_model]*100:.2f}%)\n")
        f.write(f"- Improvement: {(results[best_model] - results[worst_model])*100:.2f}%\n")
        
        f.write("\n")
        f.write("Conclusions:\n")
        f.write("- GRUå’ŒLSTMé€šè¿‡é—¨æ§æœºåˆ¶ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜\n")
        f.write("- LSTMçš„cell stateæä¾›äº†æ›´å¼ºçš„é•¿æœŸè®°å¿†èƒ½åŠ›\n")
        f.write("- ä½†åœ¨CIFAR-10è¿™ç§å›¾åƒä»»åŠ¡ä¸Šï¼Œæ”¹è¿›å¹…åº¦æœ‰é™\n")
        f.write("- åŸå› ï¼šrow-by-rowåºåˆ—åŒ–ç ´åäº†2Dç©ºé—´ç»“æ„\n")
        f.write("=" * 70 + "\n")
    
    print(f"âœ“ Saved: {save_path.name}")


def main():
    print("\n" + "=" * 70)
    print("ğŸ“Š RNN Variants Comparison Experiment")
    print("=" * 70 + "\n")
    
    results_dir = current_dir / "comparison_results"
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†å·²æœ‰ç»“æœ
    print("[1/3] Collecting experiment results...")
    
    rnn_hist = load_history(current_dir / "cifar10_results" / "history.npy")
    gru_hist = load_history(current_dir / "gru_results" / "gru_history.npy")
    lstm_hist = load_history(current_dir / "lstm_results" / "lstm_history.npy")
    
    histories = {
        "Vanilla RNN": rnn_hist,
        "GRU": gru_hist,
        "LSTM": lstm_hist,
    }
    
    # æ£€æŸ¥å“ªäº›å®éªŒå·²å®Œæˆ
    available = {name: hist is not None for name, hist in histories.items()}
    print(f"   Vanilla RNN: {'âœ“' if available['Vanilla RNN'] else 'âœ—'}")
    print(f"   GRU:         {'âœ“' if available['GRU'] else 'âœ—'}")
    print(f"   LSTM:        {'âœ“' if available['LSTM'] else 'âœ—'}")
    
    if not any(available.values()):
        print("\nâŒ No experiment results found!")
        print("Please run the following experiments first:")
        print("   - python -m rnn.experiments.cifar10_experiment")
        print("   - python -m rnn.experiments.gru_experiment")
        print("   - python -m rnn.experiments.lstm_experiment")
        return
    
    # åŠ è½½æµ‹è¯•é›†å‡†ç¡®ç‡
    print("\n[2/3] Loading test accuracies...")
    results = {}
    
    # RNN
    rnn_report = current_dir / "cifar10_results" / "rnn_experiment_results.txt"
    if rnn_report.exists():
        with open(rnn_report, 'r') as f:
            for line in f:
                if "Test Accuracy:" in line:
                    acc = float(line.split(":")[1].strip())
                    results["Vanilla RNN"] = acc
                    print(f"   Vanilla RNN: {acc:.4f}")
                    break
    
    # GRU
    gru_report = current_dir / "gru_results" / "gru_experiment_results.txt"
    if gru_report.exists():
        with open(gru_report, 'r') as f:
            for line in f:
                if "Test Accuracy:" in line:
                    acc = float(line.split(":")[1].strip())
                    results["GRU"] = acc
                    print(f"   GRU:         {acc:.4f}")
                    break
    
    # LSTM
    lstm_report = current_dir / "lstm_results" / "lstm_experiment_results.txt"
    if lstm_report.exists():
        with open(lstm_report, 'r') as f:
            for line in f:
                if "Test Accuracy:" in line:
                    acc = float(line.split(":")[1].strip())
                    results["LSTM"] = acc
                    print(f"   LSTM:        {acc:.4f}")
                    break
    
    if len(results) == 0:
        print("\nâŒ Could not load any test results!")
        return
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    print("\n[3/3] Generating comparison visualizations...")
    plot_comparison(histories, results_dir)
    
    if len(results) > 0:
        plot_final_results(results, results_dir)
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = results_dir / "comparison_report.txt"
    generate_report(results, histories, report_path)
    
    print("\n" + "=" * 70)
    print("âœ… Comparison Complete!")
    print(f"ğŸ“ Results saved to: {results_dir}")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    main()