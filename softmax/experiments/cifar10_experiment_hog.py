"""
CIFAR-10 Softmaxåˆ†ç±»å™¨å®éªŒ (HOG ç‰¹å¾ç‰ˆ)

å®Œæ•´çš„å®éªŒæµç¨‹ï¼š
1. åŠ è½½CIFAR-10æ•°æ®é›†
2. æ•°æ®é¢„å¤„ç† (æå–HOGç‰¹å¾)
3. è®­ç»ƒSoftmaxåˆ†ç±»å™¨
4. è¯„ä¼°æ¨¡å‹æ€§èƒ½
5. ç”Ÿæˆå¯è§†åŒ–ç»“æœ (åŒ…æ‹¬æ··æ·†çŸ©é˜µ)
"""

import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np
import matplotlib.pyplot as plt
from softmax_classifier import SoftmaxClassifier
from cifar10_utils_hog import load_cifar10, preprocess_data, get_cifar10_class_names

# æ–°å¢å¯¼å…¥, ç”¨äºæ··æ·†çŸ©é˜µ
from sklearn.metrics import confusion_matrix
import seaborn as sns 
# ============================================================================
# å®éªŒé…ç½® - åœ¨è¿™é‡Œä¿®æ”¹æ‰€æœ‰è¶…å‚æ•°
# ============================================================================
CONFIG = {
    # æ•°æ®é›†é…ç½®
    'data': {
        'train_samples': 49000,
        'val_samples': 1000,
        'test_samples': 1000,
    },
    
    # æ¨¡å‹é…ç½®
    'model': {
        'reg_strength': 5e-4,  # L2æ­£åˆ™åŒ–å¼ºåº¦
    },
    
    # è®­ç»ƒé…ç½®
    'training': {
        'learning_rate': 0.005,      # åˆå§‹å­¦ä¹ ç‡
        'num_epochs': 1000,          # æœ€å¤§è®­ç»ƒè½®æ•°
        'batch_size': 128,           # æ‰¹æ¬¡å¤§å°
        'patience': 200,             # æ—©åœè€å¿ƒå€¼
        'print_every': 20,           # æ‰“å°é—´éš”
        'lr_decay_epochs': 150,      # å­¦ä¹ ç‡è¡°å‡é—´éš”
        'lr_decay_rate': 0.99,       # å­¦ä¹ ç‡è¡°å‡ç‡
    },
    
    # å¯è§†åŒ–é…ç½®
    'visualization': {
        'num_pred_samples': 20,      # é¢„æµ‹å¯è§†åŒ–æ ·æœ¬æ•°
    }
}
# ============================================================================


def visualize_weights(W, class_names, save_path):
    """å¯è§†åŒ–å­¦ä¹ åˆ°çš„æƒé‡æ¨¡æ¿"""
    # (æ³¨æ„: HOG ç‰¹å¾æƒé‡ (325ç»´) æ— æ³•è¢« reshape ä¸º (32, 32, 3))
    # (æ­¤å‡½æ•°åœ¨ HOG å®éªŒä¸­ä¸åº”è¢«è°ƒç”¨)
    print("âš ï¸ è­¦å‘Š: visualize_weights æ— æ³•ç”¨äº HOG ç‰¹å¾ã€‚è·³è¿‡...")
    return 


def plot_training_curves(history, save_path):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    epochs = history['epochs']
    
    # æŸå¤±æ›²çº¿ - è½¬æ¢å­—å…¸ä¸ºåˆ—è¡¨
    loss_epochs = sorted(history['loss_history'].keys())
    loss_values = [history['loss_history'][e] for e in loss_epochs]
    
    ax1.plot(loss_values, 'b-', linewidth=2, alpha=0.8)
    ax1.set_xlabel('Iteration', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training Loss over Time', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(0, len(loss_values))
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(epochs, history['train_acc_history'], 'b-', 
             label='Training Accuracy', linewidth=2, alpha=0.8)
    ax2.plot(epochs, history['val_acc_history'], 'r-', 
             label='Validation Accuracy', linewidth=2, alpha=0.8)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy', fontsize=12)
    ax2.set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=11, loc='lower right')
    ax2.grid(True, alpha=0.3)
    ax2.set_xlim(0, max(epochs))
    ax2.set_ylim(0, 1)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()

def visualize_predictions(X_raw, y_test, classifier, X_processed, class_names, save_path, num_samples=20):
    """å¯è§†åŒ–æ¨¡å‹é¢„æµ‹ç»“æœ"""
    indices = np.random.choice(len(X_raw), num_samples, replace=False)
    
    fig, axes = plt.subplots(4, 5, figsize=(15, 12))
    axes = axes.flatten()
    
    for idx, ax in enumerate(axes):
        i = indices[idx]
        
        # ä½¿ç”¨ X_raw (åŸå§‹åƒç´ ) æ¥æ˜¾ç¤ºå›¾åƒ
        img = X_raw[i] # X_raw å·²ç»æ˜¯ (32, 32, 3)
        img = (img - img.min()) / (img.max() - img.min() + 1e-5)
        img = np.clip(img, 0, 1)
        
        # ä½¿ç”¨ X_processed (HOGç‰¹å¾) æ¥è¿›è¡Œé¢„æµ‹
        pred = classifier.predict(X_processed[i:i+1])[0]
        true = y_test[i]
        
        ax.imshow(img)
        color = 'green' if pred == true else 'red'
        
        ax.set_title(f'True: {class_names[true]}\nPred: {class_names[pred]}',
               color=color, fontsize=10, fontweight='bold')

        ax.axis('off')
    
    plt.suptitle('Sample Predictions (Green=Correct, Red=Wrong)', 
                 fontsize=14, fontweight='bold', y=0.995)
    plt.tight_layout(pad=0.5, h_pad=1.0) # è°ƒæ•´å¸ƒå±€é˜²æ­¢æ ‡é¢˜é‡å 
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ é¢„æµ‹å¯è§†åŒ–å·²ä¿å­˜: {save_path.name}")


def plot_confusion_matrix(y_true, y_pred, class_names, save_path):
    """
    ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
    """
    print(f"ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
    
    # 1. è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    
    # 2. å½’ä¸€åŒ– (æŒ‰è¡Œ, æ˜¾ç¤ºå¬å›ç‡/çœŸå®ä¸ºAçš„, æœ‰å¤šå°‘è¢«é¢„æµ‹ä¸ºB)
    # åŠ  1e-6 é¿å…é™¤é›¶
    cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-6)

    plt.figure(figsize=(10, 8))
    
    # 3. ä½¿ç”¨ Seaborn ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(cm_normalized, 
                annot=True,     # åœ¨æ ¼å­é‡Œæ˜¾ç¤ºæ•°å­—
                fmt='.2f',      # æ•°å­—æ ¼å¼ (ä¸¤ä½å°æ•°)
                cmap='Blues',   # é¢œè‰²
                xticklabels=class_names,
                yticklabels=class_names)
    
    plt.ylabel('True Label (çœŸå®ç±»åˆ«)', fontsize=13)
    plt.xlabel('Predicted Label (é¢„æµ‹ç±»åˆ«)', fontsize=13)
    plt.title('Confusion Matrix (Normalized by True Label)', fontsize=16, fontweight='bold')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.close()
    print(f"âœ“ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path.name}")


def print_config():
    """æ‰“å°å½“å‰é…ç½®"""
    print("" + "=" * 70)
    print("å®éªŒé…ç½®:")
    print("=" * 70)
    
    print("ğŸ“Š æ•°æ®é›†:")
    for key, value in CONFIG['data'].items():
        print(f"  â€¢ {key}: {value}")
    
    print("ğŸ§  æ¨¡å‹:")
    for key, value in CONFIG['model'].items():
        print(f"  â€¢ {key}: {value}")
    
    print("ğŸ¯ è®­ç»ƒ:")
    for key, value in CONFIG['training'].items():
        print(f"  â€¢ {key}: {value}")
    
    print("ğŸ¨ å¯è§†åŒ–:")
    for key, value in CONFIG['visualization'].items():
        print(f"  â€¢ {key}: {value}")
    
    print("=" * 70)


def main():
    """ä¸»å®éªŒæµç¨‹"""
    print("=" * 70)
    print(" " * 15 + "CIFAR-10 Softmaxåˆ†ç±»å™¨å®éªŒ (HOG ç‰¹å¾ç‰ˆ)")
    print("=" * 70)
    
    # æ‰“å°é…ç½®
    print_config()
    
    # è®¾ç½®ç»“æœç›®å½•
    results_dir = Path(__file__).parent / 'cifar10_hog_results'
    results_dir.mkdir(exist_ok=True)
    
    # ==================== 1. åŠ è½½æ•°æ® ====================
    print("[1/6] åŠ è½½CIFAR-10æ•°æ®é›†...")
    print("-" * 70)
    
    # ä¿ç•™åŸå§‹å›¾åƒæ•°æ®, ç”¨äºå¯è§†åŒ–
    X_train_raw, y_train, X_val_raw, y_val, X_test_raw, y_test = load_cifar10(
        data_dir='../../data/cifar-10-batches-py',
        train_samples=CONFIG['data']['train_samples'],
        val_samples=CONFIG['data']['val_samples'],
        test_samples=CONFIG['data']['test_samples']
    )
    print(f"âœ“ è®­ç»ƒé›†: {len(X_train_raw)} æ ·æœ¬")
    print(f"âœ“ éªŒè¯é›†: {len(X_val_raw)} æ ·æœ¬")
    print(f"âœ“ æµ‹è¯•é›†: {len(X_test_raw)} æ ·æœ¬")
    print(f"âœ“ å›¾åƒå°ºå¯¸: {X_train_raw.shape[1:]}")
    
    # ==================== 2. é¢„å¤„ç† ====================
    print("[2/6] æ•°æ®é¢„å¤„ç†...")
    print("-" * 70)
    
    # X_train, X_val, X_test ç°åœ¨æ˜¯ HOG ç‰¹å¾
    X_train, X_val, X_test = preprocess_data(X_train_raw, X_val_raw, X_test_raw)
    
    # æ‰“å°ä¿¡æ¯æ¥è‡ª HOG ç‰ˆ utils
    print(f"âœ“ å±•å¹³åç‰¹å¾ç»´åº¦: {X_train.shape[1]} (HOGç‰¹å¾ + 1åç½®)") 
    print(f"âœ“ æ•°æ®å·²ä¸­å¿ƒåŒ–ï¼ˆå‡å»è®­ç»ƒé›†å‡å€¼ï¼‰")
    print(f"âœ“ å·²æ·»åŠ åç½®é¡¹")
    
    # ==================== 3. åˆ›å»ºåˆ†ç±»å™¨ ====================
    print("[3/6] åˆ›å»ºSoftmaxåˆ†ç±»å™¨...")
    print("-" * 70)
    classifier = SoftmaxClassifier(
        num_features=X_train.shape[1], # HOG ç‰¹å¾ç»´åº¦ (e.g., 325)
        num_classes=10,
        reg_strength=CONFIG['model']['reg_strength']
    )
    print(f"âœ“ è¾“å…¥ç‰¹å¾æ•°: {classifier.num_features}")
    print(f"âœ“ è¾“å‡ºç±»åˆ«æ•°: {classifier.num_classes}")
    print(f"âœ“ æƒé‡å‚æ•°é‡: {classifier.W.size:,}")
    print(f"âœ“ æ­£åˆ™åŒ–å¼ºåº¦: {classifier.reg_strength}")
    
    # ==================== 4. è®­ç»ƒ ====================
    print("[4/6] å¼€å§‹è®­ç»ƒ...")
    print("=" * 70)
    
    history = classifier.train(
        X_train, y_train,
        X_val, y_val,
        learning_rate=CONFIG['training']['learning_rate'],
        num_epochs=CONFIG['training']['num_epochs'],
        batch_size=CONFIG['training']['batch_size'],
        patience=CONFIG['training']['patience'],
        verbose=True,
        print_every=CONFIG['training']['print_every']
    )
    
    # ==================== 5. è¯„ä¼° ====================
    print("[5/6] è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    print("=" * 70)
    train_acc = classifier.evaluate(X_train, y_train)
    val_acc = classifier.evaluate(X_val, y_val)
    test_acc = classifier.evaluate(X_test, y_test)
    
    print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f} ({train_acc*100:.2f}%)")
    print(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f} ({val_acc*100:.2f}%)")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)")
    print("=" * 70)
    
    # ==================== 6. ä¿å­˜ç»“æœ ====================
    print("[6/6] ä¿å­˜å®éªŒç»“æœ...")
    print("-" * 70)
    
    # 6.1 ä¿å­˜æ¨¡å‹æƒé‡
    model_path = results_dir / 'softmax_classifier.npy'
    classifier.save_model(str(model_path))
    print(f"âœ“ æ¨¡å‹æƒé‡å·²ä¿å­˜: {model_path.name}")
    
    # 6.2 ä¿å­˜æ–‡æœ¬ç»“æœï¼ˆåŒ…å«é…ç½®ï¼‰
    results_file = results_dir / 'cifar10_experiment_results.txt'
    with open(results_file, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write(" " * 15 + "CIFAR-10 Softmaxåˆ†ç±»å™¨å®éªŒ (HOG ç‰¹å¾)\n")
        f.write("=" * 70 + "\n\n")
        
        # ä¿å­˜é…ç½®
        f.write("å®éªŒé…ç½®:\n")
        f.write("-" * 70 + "\n")
        f.write("ğŸ“Š æ•°æ®é›†:\n")
        for key, value in CONFIG['data'].items():
            f.write(f"   â€¢ {key}: {value}\n")
        f.write("ğŸ§  æ¨¡å‹:\n")
        for key, value in CONFIG['model'].items():
            f.write(f"   â€¢ {key}: {value}\n")
        f.write("ğŸ¯ è®­ç»ƒ:\n")
        for key, value in CONFIG['training'].items():
            f.write(f"   â€¢ {key}: {value}\n")
        f.write("\n" + "-" * 70 + "\n\n")
        
        f.write("æ•°æ®é›†ä¿¡æ¯:\n")
        f.write(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬\n")
        f.write(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬\n")
        f.write(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬\n")
        f.write(f"   ç‰¹å¾ç»´åº¦: {X_train.shape[1]} (HOG + åç½®)\n\n")
        
        f.write("æœ€ç»ˆæ€§èƒ½:\n")
        f.write(f"   è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f} ({train_acc*100:.2f}%)\n")
        f.write(f"   éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f} ({val_acc*100:.2f}%)\n")
        f.write(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)\n\n")
        
        f.write("è®­ç»ƒå†å²:\n")
        f.write("-" * 70 + "\n")
        f.write(f"{'Epoch':<10}{'Loss':<15}{'Train Acc':<15}{'Val Acc':<15}\n")
        f.write("-" * 70 + "\n")
        for i, epoch in enumerate(history['epochs']):
            f.write(f"{epoch:<10}{history['loss_history'][epoch]:<15.4f}"
                  f"{history['train_acc_history'][i]:<15.4f}"
                  f"{history['val_acc_history'][i]:<15.4f}\n")
        
        f.write("\n" + "=" * 70 + "\n")
    
    print(f"âœ“ æ–‡æœ¬ç»“æœå·²ä¿å­˜: {results_file.name}")
    
    # 6.3 ç”Ÿæˆå¯è§†åŒ–
    print("ç”Ÿæˆå¯è§†åŒ–...")
    print("-" * 70)
    
    curves_path = results_dir / 'training_curves.png'
    plot_training_curves(history, curves_path)
    
    # HOG å®éªŒä¸ç”Ÿæˆæƒé‡å¯è§†åŒ–
    class_names = get_cifar10_class_names()
    
    pred_path = results_dir / 'cifar10_prediction_visualization.png'
    
    visualize_predictions(
        X_test_raw, y_test, classifier, X_test, # ä¼ å…¥åŸå§‹å›¾åƒå’ŒHOGç‰¹å¾
        class_names, pred_path,
        num_samples=CONFIG['visualization']['num_pred_samples']
    )
    
    # --- æ–°å¢ï¼šç”Ÿæˆæ··æ·†çŸ©é˜µ ---
    print("æ­£åœ¨è·å–æµ‹è¯•é›†æ‰€æœ‰é¢„æµ‹, ç”¨äºç”Ÿæˆæ··æ·†çŸ©é˜µ...")
    y_pred_test = classifier.predict(X_test)
    
    # å®šä¹‰è·¯å¾„å¹¶è°ƒç”¨æ–°å‡½æ•°
    cm_path = results_dir / 'confusion_matrix.png'
    plot_confusion_matrix(y_test, y_pred_test, class_names, cm_path)
    # -------------------------
    
    # ==================== å®Œæˆ ====================
    print("" + "=" * 70)
    print("âœ… å®éªŒå®Œæˆï¼")
    print("=" * 70)
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶ä½ç½®:")
    print(f"   {results_dir}/")
    print(f"   â”œâ”€â”€ cifar10_experiment_results.txt")
    print(f"   â”œâ”€â”€ training_curves.png")
    print(f"   â”œâ”€â”€ cifar10_prediction_visualization.png")
    print(f"   â”œâ”€â”€ confusion_matrix.png") # <--- å·²æ·»åŠ 
    print(f"   â””â”€â”€ softmax_classifier.npy")
    print("=" * 70)


if __name__ == '__main__':
    main()