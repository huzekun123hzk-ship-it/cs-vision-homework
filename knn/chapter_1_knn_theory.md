## 第一章：图像分类器 - K-近邻 (K-Nearest Neighbors, K-NN)

&emsp;&emsp;欢迎来到《用纯Python手搓经典计算机视觉算法》的第一站！在本章，我们将从一个极其生活化的思想出发，逐步揭开 K-近邻（K-NN）算法的神秘面纱。它不仅是机器学习领域最古老的算法之一，更是我们踏入图像分类世界最理想、最直观的向导。K-NN是一种“非参数”方法，这意味着它不对数据的内在分布做任何假设，这种灵活性使它成为一个绝佳的基准模型，帮助我们建立对“特征空间”中“距离”的核心直觉。

### 1.1 现实类比 - 从水果分类开始

&emsp;&emsp;想象一下，桌上有一个你从未见过的水果，你想知道它更可能是“苹果”还是“橙子”。一个最直观的方法是什么？你可能会观察它的大小、形状和颜色，然后和你脑海中已经认识的苹果和橙子进行比较。这个比较的过程，就是将未知水果的“特征”（颜色、大小、形状）与已知水果的特征进行匹配。

&emsp;&emsp;如果这个水果在“颜色”和“大小”这两个特征上，与 5 个你最熟悉的水果（K=5）中的 4 个苹果和 1 个橙子都非常相似，你大概率会判断它是一个苹果。在这个过程中，你实际上已经不自觉地完成了一次K-NN分类。这里的“未知水果”就是我们的**测试样本**，“已知的水果”就是**训练数据**，而“颜色”和“大小”等可供比较的属性，就是**特征向量**。

&emsp;&emsp;K-NN 算法的内核，就是基于这样一种简单、强大的“近朱者赤，近墨者黑”的思想。它通过衡量未知事物与已知事物之间的“距离”，来对未知事物进行分类。

### 1.2 计算机的视角：什么是图像分类？

&emsp;&emsp;在深入算法的核心三要素之前，我们必须先统一“语言”，理解计算机是如何“看”待这个世界的，以及它如何量化“相似度”。

&emsp;&emsp;对于人类，识别图像是天赋。但对于计算机，一张图片只是一个由像素值构成的巨大数字矩阵。例如，一张小小的 32x32 像素的彩色（RGB三通道）图片，在计算机看来就是一个 $32\times32 \times 3 = 3072$个数字组成的集合。

&emsp;&emsp;为了让算法能够处理，我们通常会把这个三维的像素矩阵“压平”（Flatten），变成一个长长的一维向量。对于上面的例子，我们就得到了一个3072维的向量。现在，我们的**图像分类任务**就可以被精确地定义为：

> **给定一个代表图像的高维向量，算法需要从一个预设的类别列表中（比如“猫”、“狗”、“飞机”），为这个向量分配合适的类别标签。**

&emsp;&emsp;我们将每张图片（即每个高维向量）看作是这个3072维“特征空间”中的一个点。那么，“相似”的图片（例如，两张不同角度拍摄的猫的照片）在空间中的位置也应该“相近”。这为我们使用“距离”来衡量相似度提供了理论基础。值得注意的是，“压平”操作会丢失像素点的空间结构信息（例如，哪些像素是相邻的），这是这类简单方法的局限性，也是后续更高级的卷积神经网络（CNN）着力解决的问题。
<p align="center"><b>图 1：图像的向量化（“Flatten”）过程</b></p>
<p align="center">
<img src="./assets/1-1_images_to_vector.png" alt="计算机如何将图像转换为向量" width="80%">
</p>
<p align="center" style="font-size: 14px; color: #6c757d; line-height: 1.5;">

> **说明：** 左侧是一个简化的$5 \times 5$单通道图像，其中红色代表前景（值为 1），灰色代表背景（值为 0）。
>经过“Flatten”操作后，它被转换为一个 25 维的一维行向量，算法将基于这个向量进行后续计算。



### 1.3 核心三要素

&emsp;&emsp;我们已经从生活例子中体会了 K-NN 的直觉思想：相似的对象往往有相似的属性。接下来，我们将从算法设计的角度，逐步把这种直觉形式化为可以计算的数学模型。要让这个简单的思想变成一个可以工作的算法，我们需要定义三个核心要素：

#### 1.3.1 距离度量 (Distance Metric)：如何定义“相似”？

&emsp;&emsp;“相似”是一个模糊的概念，在数学中，我们用“距离”来精确地衡量它。距离越近，代表相似度越高。在处理多维特征（比如一张图片的所有像素）时，最常用的两种距离计算方法是：

* **L2 距离 (欧氏距离, Euclidean Distance)**
  &emsp;&emsp;这可以想象成空间中两点间的直线距离，就像鸟的飞行路线，也是我们最熟悉、最直观的距离度量。例如，在二维空间中，点 A(1, 2) 和点 B(4, 6) 之间的 L2 距离是 $\sqrt{(4-1)^2 + (6-2)^2} = \sqrt{9+16} = 5$。因为 L2 距离计算了差值的平方，所以它对较大的差异（即异常值）非常敏感，一个维度上的巨大差异会显著影响整体距离。

* **L1 距离 (曼哈顿距离, Manhattan Distance)**
  &emsp;&emsp;这可以想象成在城市网格状的街道上，从一个十字路口走到另一个十字路口需要走过的街区总长度。它衡量的是两个点在标准坐标轴上的绝对轴距总和。对于同样的点 A(1, 2) 和 B(4, 6)，L1 距离是 $|4-1| + |6-2| = 3+4 = 7$。由于 L1 距离不进行平方操作，它对所有维度的差异都一视同仁，因此对异常值的鲁棒性比L2距离更好。

&emsp;&emsp;选择哪种距离度量，取决于数据的特征和具体的应用场景。对于图像像素这类同质化的特征，两者通常都可以取得不错的效果。

#### 1.3.2 K 值的选择：找多少个“邻居”？

&emsp;&emsp;K 值的选择，即邻居的数量，对模型的最终预测结果有决定性的影响。它是一个需要我们手动设置的“超参数”（Hyperparameter），直接关系到模型的复杂度和泛化能力。

* **K 值过小**：模型会变得非常敏感，容易受到噪声数据的影响。如果 K=1，模型仅仅依赖于最近的一个邻居。想象一个被错误标记的训练样本（一张猫的图片被标为“狗”），如果一个新的测试样本恰好离这个错误样本最近，1-NN分类器就会自信地做出错误判断。这会导致模型学习了训练集中的噪声和特例，而不是数据的整体规律，我们称之为**过拟合 (Overfitting)**。

* **K 值过大**：模型会变得非常“迟钝”，倾向于忽略数据中局部的、细微的特征。如果 K 值等于全体样本数量，那无论新样本是什么，预测结果都会是训练集中数量最多的那个类别，这显然不是我们想要的。过大的K值会使决策边界过于平滑，无法捕捉数据的复杂结构，我们称之为**欠拟合 (Underfitting)**。


&emsp;&emsp;选择一个合适的 K 值至关重要。在实际应用中，我们不能用测试集来选择K值（因为这相当于“作弊”），而是通常会采用一种叫做**交叉验证 (Cross-Validation)** 的方法：从训练集中分出一部分作为“验证集”，用剩余的训练数据训练模型，然后在验证集上评估不同K值的表现，最终选择表现最好的那个K值。为了避免投票时出现平局，K值通常被选为奇数。

<p align="center"><b>图 2：K 值对预测结果的影响（同一测试点）</b></p>

<div style="display: flex; justify-content: center; align-items: center; gap: 12px;">
  <figure style="text-align: center;">
    <img src="./assets/1-2_knn_k3.png" alt="K=3 邻域与预测（结果 B）" width="100%">
    <figcaption>图 2 (a)：K = 3 时预测结果 B</figcaption>
  </figure>
  <figure style="text-align: center;">
    <img src="./assets/1-3_knn_k5.png" alt="K=5 邻域与预测（结果 A）" width="100%">
    <figcaption>图 2 (b)：K = 5 时预测结果 A</figcaption>
  </figure>
</div>

> **说明：** 绿色星形为测试点；圆点为 Class A，叉号为 Class B；虚线圆表示到第 K 个邻居的距离。  
> K 越小，模型越“敏感”（可能过拟合）；K 越大，模型越“平滑”（可能欠拟合）。

### 1.3.3 分类决策规则 (Decision Rule)：如何根据邻居做决定？

&emsp;&emsp;找到了 K 个最相似的“邻居”后，我们就需要一个规则来做出最终的判断。

* **多数投票 (Majority Vote)**: 这是最常用、最简单的规则。在 K 个邻居中，分别统计每个类别的数量，数量最多的那个类别，就是我们对新样本的预测类别。每个邻居的“话语权”都是平等的。

* **距离加权投票 (Distance-weighted Voting)**: 一种更精细的策略。它认为，距离更近的邻居应该有更大的影响力。因此，每个邻居的投票权重可以设置为其距离的倒数（例如 $1/d$）。这样，即使在K个邻居中，某个类别的数量不占优，但如果属于该类别的邻居都离测试点非常近，它依然有可能胜出。这种方法可以有效减小K值选择对结果的影响。

### 1.4 算法的数学描述

&emsp;&emsp;现在，我们用更严谨的数学语言来统一描述 K-NN 的完整流程。

#### 1.4.1 距离度量公式

&emsp;&emsp;假设我们有两个 n 维的样本点，$x = (x_1, x_2, ..., x_n)$ 和 $y = (y_1, y_2, ..., y_n)$：

* **L2 距离 (欧氏距离)** 的计算公式为：

  $$
  d_2(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
  $$

* **L1 距离 (曼哈顿距离)** 的计算公式为：

  $$
  d_1(x, y) = \sum_{i=1}^{n} |x_i - y_i|
  $$

* **更一般地，Lp 距离 (闵可夫斯基距离) 的定义为**：

  $$
  d_p(x, y) = \left(\sum_{i=1}^{n} |x_i - y_i|^p\right)^{1/p}, \quad p \ge 1
  $$

  其中，当 $p=1$ 时为曼哈顿距离，当 $p=2$ 时为欧氏距离。

<p align="center"><b>图 3：L1（曼哈顿）与 L2（欧氏）等距曲线对比</b></p>

<div style="display: flex; justify-content: center; align-items: center;">
  <figure style="text-align: center; width: 50%;">
    <img src="./assets/1-4_lp_iso_curves.png" alt="L1 与 L2 等距曲线对比" style="width:100%; border:1px solid #ccc; border-radius:6px;">

  </figure>
</div>

> **说明：**  
> L2（欧氏距离）的等距曲线为**圆形**，表示在所有方向上距离权重相同；  
> L1（曼哈顿距离）的等距曲线为**菱形**，表示沿坐标轴方向的距离占主导。  
> 因此，不同的距离度量会改变“邻域”的形状，进而影响 $N_k(x)$ 的组成和最终分类结果。


#### 1.4.2 分类决策的数学表达

&emsp;&emsp;给定一个测试样本 $x_{test}$，以及包含 $N$ 个样本的训练集 $D = \{(x_1, y_1), ..., (x_N, y_N)\}$，其中 $y_i$ 是样本 $x_i$ 的类别标签。K-NN 的预测过程可以分为以下两步：

1. **寻找邻居**：在训练集 $D$ 中，找到距离 $x_{test}$ 最近的 $K$ 个样本点，构成邻居集合 $N_k(x_{test})$。

2. **投票决策**：通过多数投票法做出预测，选择邻居集合中出现次数最多的类别作为预测结果 $\hat{y}$：

   $$
   \hat{y} = \underset{c}{\operatorname{argmax}} \sum_{(x_i, y_i) \in N_k(x_{test})} I(y_i = c)
   $$

   &emsp;&emsp;这个公式看起来复杂，但它的含义非常直白：“对于每一个可能的类别 `c`，我们去统计邻居集合 $N_k(x_{test})$ 中有多少样本的标签 $y_i$ 等于 `c`（$I(\cdot)$ 是指示函数，条件成立时为 1，否则为 0）。最后，我们选择那个让总和最大的类别作为最终的预测结果 $\hat{y}$。”

### 1.5 算法实现流程

&emsp;&emsp;K-NN 分类器的预测过程可以形式化地描述为以下算法。该算法的输入为一个测试样本，输出为其预测的类别标签。

---
**算法 1：K-近邻分类算法**

**输入：**

* 测试样本 $x_{test}$

* 训练集 $D = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$

* 邻居数 $K$

* 距离度量函数 $d(\cdot, \cdot)$

**输出：**

* 测试样本的预测类别 $\hat{y}$

**方法：**

1. **初始化距离集合**：创建一个空列表 $L$。

2. **计算距离**：对于 $i$ 从 $1$ 到 $N$，执行以下操作：
   a.  计算 $x_{test}$ 与训练样本 $x_i$ 之间的距离 $dist = d(x_{test}, x_i)$。这是预测阶段计算开销最大的部分，总时间复杂度为 $O(N \cdot D)$，其中 $D$ 是数据维度。
   b.  将序对 $(dist, y_i)$ 存入列表 $L$。

3. **排序**：根据距离 $dist$ 对列表 $L$ 进行升序排序。这一步的时间复杂度通常为 $O(N \log N)$。

4. **确定邻居**：选取排序后列表 $L$ 的前 $K$ 个元素，构成最近邻集合 $N_k$。

5. **投票决策**：在集合 $N_k$ 中，通过多数投票法（或加权投票法）确定出现次数最多的类别。

6. **返回结果**：将出现次数最多的类别作为预测结果 $\hat{y}$ 并返回。

*要对整个测试集进行预测，只需对测试集中的每一个样本* $x_{test}$ *独立执行以上算法即可。*

---

&emsp;&emsp;为了提高效率，可以不必对整个列表进行完整排序，而只需找到前 K 小的元素即可，这可以通过更高效的数据结构（如堆）将复杂度优化到 $O(N \log K)$。在更高级的应用中，还会使用KD树、球树等空间数据结构来加速第2步的邻居搜索过程。

### 1.6 优缺点分析

&emsp;&emsp;该算法具有简单直观的特点，其原理易于理解，实现过程简便且可解释性强。它属于“**懒惰学习**”(Lazy Learning)，在训练阶段仅需存储数据，无需进行耗时的计算。同时，它对数据分布没有特别假设，能很好地适应各种非线性可分的数据，并且天然支持多分类问题，无需任何修改便可直接应用于多分类场景。

&emsp;&emsp;此算法也存在一些不足。在预测阶段，它需要计算新样本与所有训练样本的距离，当训练集规模很大时，计算复杂度极高，非常耗时。它对**样本不平衡问题**较为敏感，若数据集中某个类别的样本数量远多于其他类别，模型往往会倾向于预测多数类。此外，在非常高维的空间中，由于所有点之间的距离可能趋向于相等，“距离”概念可能失去意义，即出现“**维度灾难**”，导致算法在高维数据中失效。而且，该算法对特征缩放敏感，若不同特征的数值范围差异巨大，数值范围大的特征会在距离计算中占据主-导，因此通常需要先对数据进行标准化处理。
### 1.7 小结与拓展

&emsp;&emsp;K-NN 算法通过“少数服从多数”的邻居投票策略，为我们提供了一种最直观、最简单的分类方法。理解 K-NN 不仅能帮助我们掌握一种实用的分类算法，更为后续学习更复杂的模型打下了坚实基础。

#### 拓展思考

1. **非参数模型的意义**：K-NN 是一个典型的非参数模型。它不对数据的内在分布做任何假设，模型复杂度会随着训练数据的增多而增加。它的“模型”就是全部训练数据本身。这与参数模型（如线性回归）形成对比，后者将数据的所有信息压缩到一组固定数量的参数（权重）中，无论数据量多大，模型大小不变。

2. **概率视角**：如果将邻居的投票比例视为对各类别的后验概率估计，K-NN 也可以被理解为一种基于经验分布的、非常直接的概率分类器。例如，若K=5的邻居中有3个是猫，2个是狗，我们可以认为该样本是猫的概率为60%，是狗的概率为40%。这为我们提供了预测的置信度。

3. **维度灾难的本质**：随着维度的增加，高维空间会变得极其稀疏，任意两点之间的距离差异会变得不明显（都很大），从而削弱了 K-NN 算法赖以生存的“邻近”概念的区分能力。应对维度灾难的常用方法包括特征选择（挑选最重要的特征）和特征提取/降回维（如使用主成分分析PCA等方法将高维数据投影到低维空间）。